---
title: "Wholesale customers clustering"
author: "Raheem Bukhsh"
date: "`r Sys.Date()`"
output:
  pdf_document:
    df_print: kable
    citation_package: natbib
    latex_engine: xelatex
    fig_crop: no
    number_sections: true
    toc: true
    toc_depth: 3
  urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# **Introduction:**   
This is 2nd project of capstone HarvardX for Data Science Professional Certificate. In this project we will analyze a dataset from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Wholesale+customers). This data set contains information about clients of a wholesale distributor. It includes annual spending in monetary units (m.u) for different product categories.  

## **Objective:**  
The objective of this project is to classify and cluster different segments of wholesale customers through unsupervised learning methods, based on their spending prefrences.   

## **Unsupervised Learning:**  
Unsupervised learning is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses.   
The algorithms are thus allowed to classify, label and/or group the data points contained within the data sets without having any external guidance in performing that task.  
The most common unsupervised learning method is cluster analysis, which is used for exploratory data analysis to find hidden patterns or grouping in data. The clusters are modeled using a measure of similarity which is defined upon metrics such as Euclidean or probabilistic distance.  

Common clustering algorithms include:  

*  Hierarchical clustering: builds a multilevel hierarchy of clusters by creating a cluster tree  
*  k-Means clustering: partitions data into k distinct clusters based on distance to the centroid of a cluster  
*  Gaussian mixture models: models clusters as a mixture of multivariate normal density components  
*  Self-organizing maps: uses neural networks that learn the topology and distribution of the data  
*  Hidden Markov models: uses observed data to recover the sequence of states  

## **Customers Segments:**    
Customer segmentation is an important application of unsupervised learning.  
Customer segmentation/clustering is the process of dividing a braod customer base or business markets, normall consisting of existing and potential customers into sub-groups of consumers(Segments) based on type of shared characteristics. In dividing or segmenting makets, researchers typically look for common characteristics such as shared needs,common interests, similar lifestyles, alike spending patterns or even same demographic profiles. The overall aim of segmentation is to identify the *most profitable segments* with *growth potential*. Using clustering techniques, companies can identify the several segments of customers allowing them to target the potential user base.  

## **Wholesale Customers Data Set:**  
Wholesale customers is a multivariate data set with 440 observations and 8 variables which are as follows:  
1. Channel: customers Channel - Horeca (Hotel/Restaurant/Cafe) or Retail channel (Nominal)  
2. Region: customers Region - Lisnon, Oporto or Other (Nominal)  
3. Fresh: annual spending (m.u.) on fresh products (Continuous)  
4. Milk: annual spending (m.u.) on milk products (Continuous)  
5. Grocery: annual spending (m.u.)on grocery products (Continuous)  
6. Frozen: annual spending (m.u.)on frozen products (Continuous)  
7. Detergents_Paper: annual spending (m.u.) on detergents and paper products (Continuous)  
8. Delicassen: annual spending (m.u.)on and delicatessen products (Continuous)  

The data has been downladed from the url $"https://archive.ics.uci.edu/ml/datasets/Wholesale+customers"$ 


# **Data Analysis:**   
First we upload the wholesale customers data set,  
```{r}
wholesale <- read.csv("C:\\Users\\Raheem\\Downloads\\Wholesale customers data.csv")
```

## **Data Exploration:**  
Upload following packages and libraries for data exploration and clustering. 

```{r}
library(corrplot)
library(dendextend)
library(stats)
library(RCurl)
library(factoextra)
library(cluster)
library(gridExtra)
library(tidyverse)


```

### **Dimensions:**  
```{r}
dim(wholesale)

```


The wholesale customers dataset has 440 observations for 8 variables.  

### **Variables:**  
```{r}
library(tidyverse)
glimpse(wholesale)
```

It shows that 2 variables channel and region are integers but categorical and can be classified as nominal integers.  

### **Summary Statistics:**  
Summary statistics is useful insight for the continuous integers with details about Minimum, maximum, Mean, Median and Standard Deviation.  

*  **Summary of Wholesale Dataset:**  
```{r}
summary(wholesale)
```

*  **Standard deviation of variables:**  
```{r}
apply(wholesale, 2, sd)
```
Channel and Region are categorical variables thus their standard deviation can be ignored. 

###  **Channel and Region Insights:**  

*  **Channel Distribution:**  

```{r}
table(wholesale$Channel)
```
1 is abbreviated for "HoReCa"(Hotel/Restaurant/Cafe) and 2 for "Retail" show the respective customers via each channel.  

```{r}
table(wholesale$Region)
```
1 is abbreviation for "Lisbon", 2 for "Oporto" and 3 for "Other" regions. 



###  **Correlation between variables:**  
Let us further explore the dataset and check if any correlation exist between the variables.  

```{r}
library(corrplot)
ws_cor <- cor(wholesale)
corrplot(ws_cor, method = "number")
```

From the above correlation plot we can see that correlation exists between several variables but it is strongest between Detergent_Paper and Grocery.


#  **Clustering Methods:**  
Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.  

There are two widely used methods used for clustering purposes. 

##  **K Means Clustering:**  

K-Means is one of the most popular "clustering" algorithms. K-means stores $k$ centroids that it uses to define clusters. A point is considered to be in a particular cluster if it is closer to that cluster's centroid than any other centroid.


K-Means finds the best centroids by alternating between (1) assigning data points to clusters based on the current centroids (2) chosing centroids (points which are the center of a cluster) based on the current assignment of data points to clusters.  
![](D:\Current Work\Study\Wholesale Dataset\kmeansViz)   
*K-means algorithm. Training examples are shown as dots, and cluster centroids are shown as crosses. (a) Original dataset. (b) Random initial cluster centroids. (c-f) Illustration of running two iterations of k-means. In each iteration, we assign each training example to the closest cluster centroid (shown by "painting" the training examples the same color as the cluster centroid to which is assigned); then we move each cluster centroid to the mean of the points assigned to it.*   

If the $i$th observation is in the $k$th cluster, then $i\in C_{k}$. The idea behind K-means clustering is that a $good$ clustering is one for which the
$within-cluster$ $variation$ is as small as possible. The within-cluster variation for cluster $C_{k}$ is a measure $W(Ck)$ of the amount by which the observations
within a cluster differ from each other. Hence we want to solve the problem:  

$minimize$ {$\sum_{K=1}^{K}W(C_k)$}    
$C1,...,CK$  

K Means Clustering algorithm works in five steps:     
*  Specify the desired number of clusters K.  
*  The algorithm selects k objects at random from the dataset. This object is the initial cluster or mean.    
*  The closest centroid obtains the assignment of a new observation. We base this assignment on the Euclidean Distance between object and the centroid.  
*  k clusters in the data points update the centroid through calculation of the new mean values present in all the data points of the cluster. The kth cluster’s centroid has a length of p that contains means of all variables for observations in the k-th cluster. We denote the number of variables with p.  
*  Iterative minimization of the total within the sum of squares. Then through the iterative minimization of the total sum of the square, the assignment stop wavering when we achieve maximum iteration. The default value is 10 that the R software uses for the maximum iterations.   

##  **Hierarchical Clustering:**  

Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.  
Hierarchical clustering starts by treating each observation as a separate cluster. Then, it repeatedly executes the following two steps:  
*  Identify the two clusters that are closest together by measuring Euclidean distance. 
*  Merge the two most similar clusters. This iterative process continues until all the clusters are merged together.  
*  **Computing hierarchical clustring:**  
To compute the hierarchical clustering the distance matrix needs to be calculated using and put the data point to the correct cluster. There are different ways we can calculate the distance between the cluster, as given below:  
1. **Complete Linkage:**  
Maximum distance is calculated between clusters before merging.  
![](D:\Current Work\Study\Wholesale Dataset\complete-linkage.png)   

2. **Single Linkage:**  
Minimum distance calculates between the clusters before merging.  
![](D:\Current Work\Study\Wholesale Dataset\Single-Linkage.png)  

3. **Average Linkage:**  
Calculates the average distance between clusters before merging.  
![](D:\Current Work\Study\Wholesale Dataset\average-linkage.png)   

4.  **Ward Linkage:**  
Instead of measuring the distance directly, it analyzes the variance of clusters. Ward’s is said to be the most suitable method for quantitative variables. To implement this method, at each step find the pair of clusters that leads to minimum increase in total within-cluster variance after merging. This increase is a weighted squared distance between cluster centers.  
Ward’s method says that the distance between two clusters, A and B, is how much the sum of squares will increase when we merge them:

$\Delta(A,B) = \sum_{i\in A \bigcup B} ||\overrightarrow{x_i} - \overrightarrow{m}_{A \bigcup B}||^2 - \sum_{i \in A}||\overrightarrow{x_i} - \overrightarrow{m}_A||^2 -\sum_{i \in B}||\overrightarrow{x_i}- \overrightarrow{m}_B||^2 
= \frac{n_An_B}{n_A+n_B} ||\overrightarrow{m}_A- \overrightarrow{m}_B||^2$  
where $\overrightarrow{m}_j$ the center of cluster $j$, and $n_j$ is the number of points in it. Δ is called the merging cost of combining the clusters A and B. With hierarchical clustering, the sum of squares starts out at zero (because every point is in its own cluster) and then grows as we merge clusters. Ward’s method keeps this growth as small as possible.  

#  **Clustring Results and Analysis:**  
To apply the clustering model first we need to scale/standardize the data but here we have either the categorical variables channel and region in 1st and 2nd column and remaining 6 columns have the continuous numeric data (spending in $) therefore therefore no need to scale the data but just excluding the categorical variables for clustering is enough.   

```{r}
wholesale_sc <- as.matrix(scale(wholesale[3:8]))
head(wholesale_sc)
```
##  **K Means Clustering:**  

*  **Selecting the number of K's:**  

As discussed earlier first we need to find the optimal clusters,  
There are three most popular methods to determine optimal clusters, which are:   
*  **Elbow Method**  
*  **Silhouette Method**  
*  **Gap Statistics**  

**Elbow Method:**  
 We calculate the clustering algorithm for several values of k. This can be done by creating a variation within k from 1 to 10 clusters. We then calculate the total within cluster sum of square (wss). Then, we proceed to plot wss based on the number of k clusters. This plot denotes the appropriate number of clusters required in our model. In the plot, the location of a **bend** or **elbow** is the indication of the optimum number of clusters.  
The process to compute the “Elbow method” has been wrapped up in a single function (fviz_nbclust):  
```{r}
library(factoextra)
library(cluster)
library(gridExtra)

set.seed(123)

fviz_nbclust(wholesale_sc , kmeans, method = "wss")
```

The elbow or bend is not sharp and any value among 2 or 5 can be selected and we select k = 5  

**Silhouette method:**  

Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified.

The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.

The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance. Just like elbow method, this process to compute the “average silhoutte method” has been wrapped up in a single function (fviz_nbclust):

```{r}
set.seed(123)

fviz_nbclust(wholesale_sc, kmeans, method = "silhouette")
```

The optimal number of clusters defined by Silhouette method is 2.  

**Gap Statistics:**  

In 2001, researchers at Stanford University – R. Tibshirani, G.Walther and T. Hastie published the Gap Statistic Method. We can use this method to any of the clustering method like K-means, hierarchical clustering etc. Using the gap statistic, one can compare the total intracluster variation for different values of k along with their expected values under the null reference distribution of data. With the help of Monte Carlo simulations, one can produce the sample dataset. For each variable in the dataset, we can calculate the range between min(xi) and max (xj) through which we can produce values uniformly from interval lower bound to upper bound.  
For computing the gap statistics method we can utilize the clusGap function for providing gap statistic as well as standard error for a given output.We can visualize the results with fviz_gap_stat which suggests the optimal number of clusters.  

```{r}
set.seed(123)
gap_stat_clust <- clusGap(wholesale_sc, FUN = kmeans, nstart = 25,K.max = 10, B = 50)
```

```{r}
fviz_gap_stat(gap_stat_clust)
```

Gap statistic method shows that optimal number of clusters is 3. 

*  **Selecting Optimal Clusters:**  
While using the above estimates the optimal value of clusters has been 2,3 and 5. Now we have to select the most optimal number of K clusters. We can also view our results by using fviz_cluster. This provides a nice illustration of the clusters. If there are more than two dimensions (variables) fviz_cluster will perform principal component analysis (PCA) and plot the data points according to the first two principal components that explain the majority of the variance.  

```{r}
#Compute Kmeans for K=2, K=3 and K=5
k2 <- kmeans(wholesale_sc, centers = 2, nstart = 30)
k3 <- kmeans(wholesale_sc, centers = 3, nstart = 30)
k5 <- kmeans(wholesale_sc, centers = 5, nstart = 30)
#Use fviz_cluster() to compare the results
d1 <- fviz_cluster(k2, geom = "point", data = wholesale_sc) + ggtitle("k = 2")
d2 <- fviz_cluster(k3, geom = "point",  data = wholesale_sc) + ggtitle("k = 3")
d3 <- fviz_cluster(k5, geom = "point",  data = wholesale_sc) + ggtitle("k = 5")

grid.arrange(d1, d2, d3, nrow = 3)
```

From above diagram it is visible that when number of clusters is 2 and 3 it separates the data in to distinctive clusters as calculated by Silhoutte method and Gap statistic.   
Now look in further detail clusters 2 and 3.  
```{r}
print(k2)
```

```{r}
print(k3)
```

From the above details we can conclude that cluster size 3 is the most optimal number of clusters as it separates the highly variable observations in to distinctive groups. These clusters can include potentially high spending customers.  

*  **Clustering with K=3:**   
```{r}
# Compute K-Means Clustering with K =3
set.seed(123)
final_segments <- kmeans(wholesale_sc, centers = 3, nstart = 30)
print(final_segments)
```

Now visualize the final clustering when K=3
```{r}
#Visualize the final clusters
fviz_cluster(final_segments, geom = "point",  data = wholesale_sc)
```

Now we can extract the clusters and do some descriptive statistics at cluster level.   
```{r}
cluster_means <- wholesale[3:8] %>% 
  mutate(Cluster = final_segments$cluster) %>%
  group_by(Cluster) %>%
  summarise_all(list(mean))
cluster_means
```

*  **Average customer spending per segment:**   
```{r}
cluster_means %>% 
  gather(Product, MU, Fresh:Delicassen)%>%
    ggplot(aes(x=Product , y = MU, fill = Product)) + geom_col(width = 0.7) + 
              facet_grid(.~ Cluster)+ 
                      scale_fill_brewer(palette = "Accent")+
                          ylab("Customer Spending in Monetory Units") +
                    ggtitle("Average Customer Spending per Segment")+
                                    theme(axis.text.x = element_blank(),
                                          axis.ticks.x = element_blank(),
                                          axis.title.x = element_blank())
```

The above graph shows that customer segments vary a lot in spending patterns and prefrences.  

Now lets analyze the Channel and Region distribution among the customer segments.  
```{r}
# Channel and Region in Cluster 1
wholesale[1:2] %>%
  mutate(Cluster = final_segments$cluster) %>% 
  filter(Cluster == 1) %>% ungroup() %>% count(Channel, Region)
```

```{r}
# Channel and Region in Cluster 2
wholesale[1:2] %>%
  mutate(Cluster = final_segments$cluster) %>% 
  filter(Cluster == 2) %>% ungroup() %>% count(Channel, Region)
```

```{r}
# Channel and Region in Cluster 3
wholesale[1:2] %>%
  mutate(Cluster = final_segments$cluster) %>% 
  filter(Cluster == 3) %>% ungroup() %>% count(Channel, Region)
```

The region 3 contributes the most of customers for the wholesale business.   

###  **K Means Clustering analysis:**  

From above analysis we can conclude below observations or customer spending habits of each segment identified in clustered data:  

Segment 1: This segment has only Retail customers who spend more on Groceries and Milk followed by Detergents and papers and then on Fresh products.  

Segment 2: This segment contains only Hotel/Restaurant/Cafe customers who spend heavily on Fresh , Frozen followed by Milk and Groceries but not on other items. Also have highest median spending on Delicassen.These customers form a well seperated group with these spending habits.  

Segment 3: This segment consist of majority Hotel/Restaurant/Cafe customers along with Retail Customers who spend decently on Fresh followed by Groceries and Milk,but spend least on Detergents_Paper and Delicassen in all groups.  

##  **Hierarchical Clustering:**   
Hierarchical clustering builds clusters within clusters, and does not require a pre-specified number of clusters like K-means do. A hierarchical clustering can be thought of as a tree and displayed as a dendrogram; at the top there is just one cluster consisting of all the observations, and at the bottom each observation is an entire cluster. In between are varying levels of clustering.we can build the clustering with hclust.
To implement hierarchical clustering first calculate the euclidean distance for wholesale data set. The 1 and 2 columns represent the categorical variables as stated earlier too therefore we can ignore these two columns for hierarchical clustering purposes.    

```{r}
wholesale_md <- wholesale[3:8]
head(wholesale_md)

```

```{r}
# Calculate Euclidean distance for scaled data wholesale_sc
dist_ws <- dist(wholesale_md, method = "euclidean")
```

*  **Selecting Linkage Method:**  
First we shall have a look at dendoograms with different linkage methods to choose the most suitable linkage method.  

**Complete Linkage:**  
```{r}
# Generate a complete linkage analysis
hc_ws_complete <- hclust(dist_ws, method = "complete")
# Plot 
plot(hc_ws_complete)
```

**Single Linkage:**  
```{r}
# Generate a single linkage analysis
hc_ws_single <- hclust(dist_ws, method = "single")
# Plot
plot(hc_ws_single)
```

**Average Linkage:**  
```{r}
# Generate a average linkage analysis
hc_ws_average <- hclust(dist_ws, method = "average")
# Plot
plot(hc_ws_average)
```

**Ward Linkage:**  
```{r}
# Generate a ward linkage analysis
hc_ws_ward <- hclust(dist_ws, method = "ward.D")
# Plot
plot(hc_ws_ward)
```

With a view from naked eye it is evident that **Ward Linkage** method is the most suitable method for this wholesale dataset because all other linkage methods are forming clusters even for individual observations. Also the clustering patterns formed by all other linkage methods do not make any sense. It is an integral part of the hierarchical clustering method that the best linkage method is choosen based upon the observation of dendrogram with naked eye.  
Now for all the hierarchical clustering purpose we shall be using the **Ward Linkage Method**.

*  **Hierarchical clustering with ward linkage method:**  

```{r}
library(tidyverse)
hc_ws_ward <- hclust(dist_ws, method = "ward.D")
#Create a cluster assignment vector at h = 750000
cl_ws_ward <- cutree(hc_ws_ward, h = 750000)
#Generate the segmented customers data frame
Segment_ws_md <- mutate(wholesale_md, cluster = cl_ws_ward)
```

**Cutting the dendogram at height of "750000" is based upon the visual observation to create k =3. Inspite of 750000 we could have cut it at 500000 and the resultant k's will be 5 but from k means clustering we know that the optimal number of clusters is 3 therefore it has been cut intentionally at a height where it creates 3 clusters.**  

```{r}
# Count the number of customers that fall in to each cluster
count(Segment_ws_md, cluster)
```

```{r}
# Create and Color the dendrogram based on the height of cutoff
library(dendextend)
dend_ws <- as.dendrogram(hc_ws_ward)
dend_ws_color <- color_branches(dend_ws, h = 750000)
plot(dend_ws_color)
```

```{r}
segmented_clusters <- Segment_ws_md %>% 
  group_by(cluster) %>% 
  summarise_all(list(mean))
segmented_clusters
```

*  **Customer spending per cluster:**  

```{r}
segmented_clusters %>% 
  gather(Product, MU, Fresh:Delicassen)%>%
    ggplot(aes(x=Product , y = MU, fill = Product)) + geom_col(width = 0.7) + 
              facet_grid(.~ cluster)+ 
                      scale_fill_brewer(palette = "Accent")+
                          ylab("Customer Spending in Monetory Units") +
                    ggtitle("Average Customer Spending per Segment")+
                                    theme(axis.text.x = element_blank(),
                                          axis.ticks.x = element_blank(),
                                          axis.title.x = element_blank())
```

###  **Hierarchical clustering analysis:**  
From the above analysis we can analyze the spending pattern of the wholesale customers. The three defined customer segments can be categorized as follows:   

Segment 1: This segment has most of Hotel/Restaurants/Cafe customers who spend the most on Fresh items followed by spending on Grocery, Frozen and Milk whereas the spending on detergent paper and delicassan is minimum.  

Segment 2: This segment does not spend much on any of items but at the same time they are the largest portion among customers.  So they seem to be the small/medium sized whole customers from all business sectors.  

Segment 3: This segment has most of retail customers because their are spending the most on grocery and milk followed by fresh and detergent paper while frozen and delicassan are least popular among this segment.  


#  **Conclusion:**  

Any method between K means and Hierarchical clustering can be used for segmentation purpose in unsupervised learning. However this decision is relative to the data being analysed. For this specific wholesale data set both methods have been used and number of clusters have been kept to 3 in both for comparison purposes. Results generated by both methods are different from each other however for this specific data set K means clustering method seems to be more relevant. 